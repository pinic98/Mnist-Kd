# Knowledge-Distillation (Pytorch)
This project is a implementation of Knowledge distillation on Mnist dataset.
  * Framework : PyTorch
  * Dataset : Mnist

## Result : "Teacher Net" to "Student Net" distillation

Model | Test Accuracy
---|---|
Teacher Net | 99.16%
Student Net | 98.32%
**Student Net with KD** |98.48%  

## References

[Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv preprint arXiv:1503.02531 (2015).](https://arxiv.org/abs/1503.02531)

[peterliht/knowledge-distillation-pytorch](https://github.com/peterliht/knowledge-distillation-pytorch)
